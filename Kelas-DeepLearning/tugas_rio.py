# -*- coding: utf-8 -*-
"""Tugas Rio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/170mxw9NrKAP08eBsTO_1Jj5luhfUZ-1A
"""

!pip install visualkeras

import visualkeras
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import os
import matplotlib.image as mpimg
import warnings
import shutil

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Conv2D
from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D, GroupNormalization
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard
from tensorflow.keras.metrics import Precision, Recall, AUC
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras import regularizers

warnings.filterwarnings('ignore')

dataset_path = "/content/drive/MyDrive/rps-cv-images"

train_dir = os.path.join(dataset_path, 'train')
validation_dir = os.path.join(dataset_path, 'val')

for i in os.listdir(train_dir):
  print(i)

if os.path.exists(train_dir) or os.path.exists(validation_dir):
    shutil.rmtree(train_dir)
    shutil.rmtree(validation_dir)

classes = os.listdir(dataset_path)
print("Classes:", classes)

for cls in classes:
  class_path = os.path.join(dataset_path, cls)
  sample_img = os.listdir(class_path)[0]
  img = mpimg.imread(os.path.join(class_path, sample_img))
  plt.imshow(img)
  plt.title(f"Class: {cls}")
  plt.show()

"""# **Train Test Split**"""

os.mkdir(train_dir)
os.mkdir(validation_dir)

Rock_train, rock_val = train_test_split(os.listdir(os.path.join(dataset_path, "rock")), test_size=0.2)
Paper_train, paper_val = train_test_split(os.listdir(os.path.join(dataset_path, "paper")), test_size=0.2)
Scissors_train, scissors_val = train_test_split(os.listdir(os.path.join(dataset_path, "scissors")), test_size=0.2)

os.mkdir(os.path.join(train_dir, "rock"))
os.mkdir(os.path.join(train_dir, "paper"))
os.mkdir(os.path.join(train_dir, "scissors"))

os.mkdir(os.path.join(validation_dir, "rock"))
os.mkdir(os.path.join(validation_dir, "paper"))
os.mkdir(os.path.join(validation_dir, "scissors"))

train_data = {"rock" : Rock_train, "paper" : Paper_train, "scissors" : Scissors_train}
val_data = {"rock" : rock_val, "paper" : paper_val, "scissors" : scissors_val}

for Class, folder in train_data.items():
    for file in folder:
        src = os.path.join(dataset_path, Class, file)
        dst = os.path.join(train_dir,  Class, file)

        shutil.copyfile(src, dst)

for Class, folder in val_data.items():
    for file in folder:
        src = os.path.join(dataset_path,  Class, file)
        dst = os.path.join(validation_dir,  Class, file)

        shutil.copyfile(src, dst)

"""# **Data Augmentatation and Data Generator**"""

BGR_MEAN = [103.393, 116.779, 123.68]

def mean_subtraction(image):
    image[..., 0] -= BGR_MEAN[2]
    image[..., 1] -= BGR_MEAN[1]
    image[..., 2] -= BGR_MEAN[0]

    return image

train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True,)

val_datagen = ImageDataGenerator(rescale=1./255)

"""## **Alexnet**"""

train_generator_alex = train_datagen.flow_from_directory(
    train_dir,
    target_size=(227, 227),
    color_mode='rgb',
    seed=1,
    batch_size=32,
    class_mode= 'categorical',
)

val_generator_alex = val_datagen.flow_from_directory(
    validation_dir,
    target_size=(227, 227),
    color_mode='rgb',
    seed=1,
    batch_size=32,
    class_mode= 'categorical',
)

"""## **VGG16**"""

train_generator_vgg = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    seed=1,
    batch_size=32,
    class_mode= 'categorical',
)

val_generator_vgg = val_datagen.flow_from_directory(
    validation_dir,
    target_size=(224, 224),
    seed=1,
    batch_size=32,
    class_mode= 'categorical',
)

"""# **Alexnet Model**"""

alex_model = Sequential([
    Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=(227, 227, 3)),
    GroupNormalization(),
    MaxPooling2D((3, 3), strides=2),
    Conv2D(256, (5, 5), padding='same', activation='relu'),
    GroupNormalization(),
    MaxPooling2D((3, 3), strides=2),
    Conv2D(384, (3, 3), padding='same', activation='relu'),
    Conv2D(384, (3, 3), padding='same', activation='relu'),
    Conv2D(256, (3, 3), padding='same', activation='relu'),
    MaxPooling2D((3, 3), strides=2),
    Flatten(),
    Dense(4096, activation='relu'),
    Dropout(0.5),
    Dense(4096, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')
])

alex_model.summary()

visualkeras.layered_view(alex_model)

"""# **VGG16 Model**"""

vgg16_model = Sequential([
    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2), strides=(2, 2)),

    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2), strides=(2, 2)),

    Conv2D(256, (3, 3), activation='relu', padding='same'),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2), strides=(2, 2)),

    Conv2D(512, (3, 3), activation='relu', padding='same'),
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2), strides=(2, 2)),

    Conv2D(512, (3, 3), activation='relu', padding='same'),
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2), strides=(2, 2)),

    Flatten(),
    Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.5),
    Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.5),
    Dense(3, activation='softmax')
])

vgg16_model.summary()

visualkeras.layered_view(vgg16_model)

"""# **Callbacks**"""

log_dir_alex = "/content/logs/alex"
tensorboard_callback_alex = TensorBoard(log_dir=log_dir_alex, histogram_freq=1)

log_dir_vgg = "/content/logs/vgg"
tensorboard_callback_vgg = TensorBoard(log_dir=log_dir_vgg, histogram_freq=1)

class MyCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if logs.get('accuracy') > 0.95 and logs.get('val_accuracy') > 0.95:
       print('\n Val Akurasi telah mencapai target')
       self.model.stop_training = True

early_stopping = MyCallback()

reduce_lr = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.2,
    patience=5,
    min_lr=1e-6
)

metrics = ['accuracy', Precision(), Recall(), AUC(name="auc")]

"""# **Train Model**"""

alex_model.compile(loss="categorical_crossentropy",
                 optimizer=Adam(learning_rate=0.0001),
                 metrics=metrics)

history_alex = alex_model.fit(
    train_generator_alex,
    epochs=100,
    steps_per_epoch=train_generator_alex.samples // 32,
    validation_steps=val_generator_alex.samples // 32,
    validation_data=val_generator_alex,
    callbacks=[early_stopping, reduce_lr, tensorboard_callback_alex]
)

vgg16_model.compile(loss="categorical_crossentropy",
                 optimizer=Adam(learning_rate=0.0001),
                 metrics=metrics)

history_vgg16 = vgg16_model.fit(
    train_generator_vgg,
    epochs=100,
    steps_per_epoch=train_generator_vgg.samples // 32,
    validation_steps=val_generator_vgg.samples // 32,
    validation_data=val_generator_vgg,
    callbacks=[early_stopping, reduce_lr, tensorboard_callback_vgg]
)

"""# **Evaluation**"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs/alex

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs/alex

